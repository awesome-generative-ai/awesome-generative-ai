{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MOz14dIIbxwj"
      },
      "source": [
        "# 6.2 Diffusion Models - UNets\n",
        "\n",
        "In this notebook we will\n",
        "- Construct a UNet from scratch in pytorch\n",
        "- Build an MNIST dataset with noise\n",
        "- Train a simple diffusion model\n",
        "- Sample from our diffusion model\n",
        "- Compare against a pre-trained model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ww4qjP81pggn"
      },
      "source": [
        "# Understanding U-Nets and Diffusion Models\n",
        "\n",
        "## Introduction to U-Net Architecture\n",
        "\n",
        "The U-Net is a convolutional neural network architecture that was originally developed for biomedical image segmentation. Its name comes from its U-shaped architecture, which consists of a contracting path (encoder) and an expansive path (decoder), connected by a bottleneck.\n",
        "\n",
        "Key features of U-Net:\n",
        "\n",
        "1. **Encoder (Contracting Path)**: Captures context and reduces spatial dimensions.\n",
        "2. **Decoder (Expanding Path)**: Enables precise localization and increases spatial dimensions.\n",
        "3. **Skip Connections**: Connect encoder layers to corresponding decoder layers, preserving fine-grained details.\n",
        "\n",
        "U-Nets have found applications beyond image segmentation, including in generative models like diffusion models.\n",
        "\n",
        "## U-Net in Diffusion Models\n",
        "\n",
        "In the context of diffusion models, U-Nets are used to predict noise at each step of the denoising process. The architecture is particularly well-suited for this task because:\n",
        "\n",
        "- It can capture both local and global features of the image.\n",
        "- It can work with different scales of information.\n",
        "- Skip connections help in preserving important details throughout the noising/denoising process.\n",
        "\n",
        "## Key Components of Our U-Net Implementation\n",
        "\n",
        "Our implementation includes several important components:\n",
        "\n",
        "1. **DownBlock**: Reduces spatial dimensions and increases channel depth.\n",
        "2. **MidBlock**: Processes features at the bottleneck of the U-Net.\n",
        "3. **UpBlock**: Increases spatial dimensions and decreases channel depth.\n",
        "4. **Attention Mechanisms**: Allows the model to focus on important parts of the input.\n",
        "5. **Time Embedding**: Injects information about the current noise level into the model.\n",
        "\n",
        "## The Diffusion Process\n",
        "\n",
        "The diffusion process involves two main steps:\n",
        "\n",
        "1. **Forward Diffusion**: Gradually add noise to an image over a fixed number of steps.\n",
        "2. **Reverse Diffusion**: Learn to remove noise step by step, eventually reconstructing the original image.\n",
        "\n",
        "Our `LinearNoiseScheduler` class handles the logistics of adding and removing noise according to a predefined schedule.\n",
        "\n",
        "## Training and Sampling\n",
        "\n",
        "The training process involves:\n",
        "\n",
        "1. Sampling a random timestep.\n",
        "2. Adding noise to an image according to this timestep.\n",
        "3. Having the model predict the added noise.\n",
        "4. Optimizing the model to minimize the difference between predicted and actual noise.\n",
        "\n",
        "Sampling from the trained model involves:\n",
        "\n",
        "1. Starting with pure noise.\n",
        "2. Iteratively applying the model to denoise the image.\n",
        "3. Following the reverse diffusion process until a clear image is produced.\n",
        "\n",
        "## Code Structure\n",
        "\n",
        "Our implementation is structured as follows:\n",
        "\n",
        "1. **Model Architecture**: Defined in the `Unet`, `DownBlock`, `MidBlock`, and `UpBlock` classes.\n",
        "2. **Noise Scheduler**: Implemented in the `LinearNoiseScheduler` class.\n",
        "3. **Training Loop**: Encapsulated in the `train` function.\n",
        "4. **Sampling**: Implemented in the `generate_samples` function.\n",
        "5. **Main Execution**: Orchestrated in the `main` function.\n",
        "\n",
        "This structure allows for a clear separation of concerns and makes the code more modular and easier to understand and modify.\n",
        "\n",
        "By studying this implementation, you can gain a deep understanding of both U-Net architecture and diffusion models, as well as how they can be combined to create powerful generative models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DuEYBsv46cJ"
      },
      "source": [
        "### Install Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qc32NDn-ujlx",
        "outputId": "77e0e9bd-10da-48c8-d664-6f7c2cd5c40b"
      },
      "outputs": [],
      "source": [
        "!pip install -qqq torchview graphviz einops matplotlib numpy opencv_python PyYAML torch torchvision tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxB3Gpxk484g"
      },
      "source": [
        "### Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jjjb8iFcjTB4"
      },
      "outputs": [],
      "source": [
        "import os, torch, torchvision, math, gc, graphviz\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from torchvision.utils import make_grid\n",
        "from tqdm import tqdm\n",
        "from torchview import draw_graph\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import Adam\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vkv3qrJ8uTg0",
        "outputId": "dcd27bde-3e03-45e2-c9ae-3471a02214d4"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NrJcaIDD4_um"
      },
      "source": [
        "### Check For CUDA Device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_kI1Zak2jS_d",
        "outputId": "81483832-02bf-4e0a-e5e6-2b0292e8a496"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMvdPOp95CRK"
      },
      "source": [
        "### Download MNIST Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NY4R-rYGuV6u"
      },
      "outputs": [],
      "source": [
        "train_set = torchvision.datasets.MNIST(\n",
        "    \"./data/\", download=True, transform=transforms.Compose([transforms.ToTensor()])\n",
        ")\n",
        "NUM_CLASSES = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "H0qSJoTWuV41",
        "outputId": "15674827-35f4-48a3-c964-3fbb141259cd"
      },
      "outputs": [],
      "source": [
        "# Adjust for display; high w/h ratio recommended\n",
        "plt.figure(figsize=(16, 1))\n",
        "\n",
        "def show_images(dataset, num_samples=10):\n",
        "    for i, img in enumerate(dataset):\n",
        "        if i == num_samples:\n",
        "            return\n",
        "        plt.subplot(1, num_samples, i + 1)\n",
        "        plt.imshow(torch.squeeze(img[0]))\n",
        "\n",
        "show_images(train_set)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRVq_b9noaPA"
      },
      "source": [
        "### Building the UNet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-vYUOU_dmJ1"
      },
      "source": [
        "#### Time Embeddings\n",
        "The time embedding is important in diffusion models to keep track of what stage the image has been de-noised. However, since time is a scalar, it needs to be converted into the right latent dimension size to be interpreted by the model.\n",
        "To acheive this, we use a position encoding to encode the timestep."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jYDb4mMBMVcF"
      },
      "outputs": [],
      "source": [
        "def get_time_embedding(timesteps, embedding_dim):\n",
        "    \"\"\"\n",
        "    Create sinusoidal time embeddings.\n",
        "\n",
        "    :param timesteps: (B,) tensor of timesteps.\n",
        "    :param embedding_dim: Dimension of the embeddings to create.\n",
        "    :return: (B, embedding_dim) tensor of embeddings.\n",
        "    \"\"\"\n",
        "    assert embedding_dim % 2 == 0, \"Embedding dimension must be even\"\n",
        "\n",
        "    half_dim = embedding_dim // 2\n",
        "    emb = math.log(10000) / (half_dim - 1)\n",
        "    emb = torch.exp(torch.arange(half_dim, device=timesteps.device) * -emb)\n",
        "    emb = timesteps.unsqueeze(1) * emb.unsqueeze(0)\n",
        "    emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=-1)\n",
        "    return emb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVp_q35tog8a"
      },
      "source": [
        "#### Down Block\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MCuTQ_0gMW8c"
      },
      "outputs": [],
      "source": [
        "# Down Block\n",
        "class DownBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, t_emb_dim, down_sample=True, num_heads=4, num_layers=1):\n",
        "        super().__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.down_sample = down_sample\n",
        "        self.t_emb_dim = t_emb_dim\n",
        "\n",
        "        self.resnet_conv_first = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.GroupNorm(8, in_channels if i == 0 else out_channels),\n",
        "                nn.SiLU(),\n",
        "                nn.Conv2d(in_channels if i == 0 else out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
        "            ) for i in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        self.t_emb_layers = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.SiLU(),\n",
        "                nn.Linear(self.t_emb_dim, out_channels)\n",
        "            ) for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        self.resnet_conv_second = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.GroupNorm(8, out_channels),\n",
        "                nn.SiLU(),\n",
        "                nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
        "            ) for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        self.attention_norms = nn.ModuleList([nn.GroupNorm(8, out_channels) for _ in range(num_layers)])\n",
        "        self.attentions = nn.ModuleList([nn.MultiheadAttention(out_channels, num_heads, batch_first=True) for _ in range(num_layers)])\n",
        "        self.residual_input_conv = nn.ModuleList([nn.Conv2d(in_channels if i == 0 else out_channels, out_channels, kernel_size=1) for i in range(num_layers)])\n",
        "        self.down_sample_conv = nn.Conv2d(out_channels, out_channels, 4, 2, 1) if self.down_sample else nn.Identity()\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        t_emb = get_time_embedding(t, self.t_emb_dim)\n",
        "        out = x\n",
        "        for i in range(self.num_layers):\n",
        "            resnet_input = out\n",
        "            out = self.resnet_conv_first[i](out)\n",
        "            out = out + self.t_emb_layers[i](t_emb)[:, :, None, None]\n",
        "            out = self.resnet_conv_second[i](out)\n",
        "            out = out + self.residual_input_conv[i](resnet_input)\n",
        "\n",
        "            batch_size, channels, h, w = out.shape\n",
        "            in_attn = out.reshape(batch_size, channels, h * w)\n",
        "            in_attn = self.attention_norms[i](in_attn)\n",
        "            in_attn = in_attn.transpose(1, 2)\n",
        "            out_attn, _ = self.attentions[i](in_attn, in_attn, in_attn)\n",
        "            out_attn = out_attn.transpose(1, 2).reshape(batch_size, channels, h, w)\n",
        "            out = out + out_attn\n",
        "\n",
        "        out = self.down_sample_conv(out)\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVwOCT9oojba"
      },
      "source": [
        "#### Mid Block\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wr8jvGZhMYej"
      },
      "outputs": [],
      "source": [
        "# Mid Block\n",
        "class MidBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, t_emb_dim, num_heads=4, num_layers=1):\n",
        "        super().__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.t_emb_dim = t_emb_dim\n",
        "\n",
        "        self.resnet_conv_first = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.GroupNorm(8, in_channels if i == 0 else out_channels),\n",
        "                nn.SiLU(),\n",
        "                nn.Conv2d(in_channels if i == 0 else out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
        "            ) for i in range(num_layers + 1)\n",
        "        ])\n",
        "\n",
        "        self.t_emb_layers = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.SiLU(),\n",
        "                nn.Linear(self.t_emb_dim, out_channels)\n",
        "            ) for _ in range(num_layers + 1)\n",
        "        ])\n",
        "\n",
        "        self.resnet_conv_second = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.GroupNorm(8, out_channels),\n",
        "                nn.SiLU(),\n",
        "                nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
        "            ) for _ in range(num_layers + 1)\n",
        "        ])\n",
        "\n",
        "        self.attention_norms = nn.ModuleList([nn.GroupNorm(8, out_channels) for _ in range(num_layers)])\n",
        "        self.attentions = nn.ModuleList([nn.MultiheadAttention(out_channels, num_heads, batch_first=True) for _ in range(num_layers)])\n",
        "        self.residual_input_conv = nn.ModuleList([nn.Conv2d(in_channels if i == 0 else out_channels, out_channels, kernel_size=1) for i in range(num_layers + 1)])\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        t_emb = get_time_embedding(t, self.t_emb_dim)\n",
        "        out = x\n",
        "\n",
        "        resnet_input = out\n",
        "        out = self.resnet_conv_first[0](out)\n",
        "        out = out + self.t_emb_layers[0](t_emb)[:, :, None, None]\n",
        "        out = self.resnet_conv_second[0](out)\n",
        "        out = out + self.residual_input_conv[0](resnet_input)\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            batch_size, channels, h, w = out.shape\n",
        "            in_attn = out.reshape(batch_size, channels, h * w)\n",
        "            in_attn = self.attention_norms[i](in_attn)\n",
        "            in_attn = in_attn.transpose(1, 2)\n",
        "            out_attn, _ = self.attentions[i](in_attn, in_attn, in_attn)\n",
        "            out_attn = out_attn.transpose(1, 2).reshape(batch_size, channels, h, w)\n",
        "            out = out + out_attn\n",
        "\n",
        "            resnet_input = out\n",
        "            out = self.resnet_conv_first[i + 1](out)\n",
        "            out = out + self.t_emb_layers[i + 1](t_emb)[:, :, None, None]\n",
        "            out = self.resnet_conv_second[i + 1](out)\n",
        "            out = out + self.residual_input_conv[i + 1](resnet_input)\n",
        "\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYl6pR1AooG0"
      },
      "source": [
        "#### Up Block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BQjOLfFlMY_F"
      },
      "outputs": [],
      "source": [
        "# Up Block\n",
        "class UpBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, t_emb_dim, up_sample=True, num_heads=4, num_layers=1):\n",
        "        super().__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.up_sample = up_sample\n",
        "        self.t_emb_dim = t_emb_dim\n",
        "\n",
        "        self.resnet_conv_first = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.GroupNorm(8, in_channels if i == 0 else out_channels),\n",
        "                nn.SiLU(),\n",
        "                nn.Conv2d(in_channels if i == 0 else out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
        "            ) for i in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        self.t_emb_layers = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.SiLU(),\n",
        "                nn.Linear(t_emb_dim, out_channels)\n",
        "            ) for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        self.resnet_conv_second = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.GroupNorm(8, out_channels),\n",
        "                nn.SiLU(),\n",
        "                nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
        "            ) for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        self.attention_norms = nn.ModuleList([nn.GroupNorm(8, out_channels) for _ in range(num_layers)])\n",
        "        self.attentions = nn.ModuleList([nn.MultiheadAttention(out_channels, num_heads, batch_first=True) for _ in range(num_layers)])\n",
        "        self.residual_input_conv = nn.ModuleList([nn.Conv2d(in_channels if i == 0 else out_channels, out_channels, kernel_size=1) for i in range(num_layers)])\n",
        "        self.up_sample_conv = nn.ConvTranspose2d(in_channels // 2, in_channels // 2, 4, 2, 1) if self.up_sample else nn.Identity()\n",
        "\n",
        "    def forward(self, x, out_down, t):\n",
        "        t_emb = get_time_embedding(t, self.t_emb_dim)\n",
        "        x = self.up_sample_conv(x)\n",
        "        x = torch.cat([x, out_down], dim=1)\n",
        "\n",
        "        out = x\n",
        "        for i in range(self.num_layers):\n",
        "            resnet_input = out\n",
        "            out = self.resnet_conv_first[i](out)\n",
        "            out = out + self.t_emb_layers[i](t_emb)[:, :, None, None]\n",
        "            out = self.resnet_conv_second[i](out)\n",
        "            out = out + self.residual_input_conv[i](resnet_input)\n",
        "\n",
        "            batch_size, channels, h, w = out.shape\n",
        "            in_attn = out.reshape(batch_size, channels, h * w)\n",
        "            in_attn = self.attention_norms[i](in_attn)\n",
        "            in_attn = in_attn.transpose(1, 2)\n",
        "            out_attn, _ = self.attentions[i](in_attn, in_attn, in_attn)\n",
        "            out_attn = out_attn.transpose(1, 2).reshape(batch_size, channels, h, w)\n",
        "            out = out + out_attn\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8p_fAqyGorfA"
      },
      "source": [
        "#### The UNet\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6kUQxWNmMZTv"
      },
      "outputs": [],
      "source": [
        "# UNet\n",
        "class Unet(nn.Module):\n",
        "    def __init__(self, model_config):\n",
        "        super().__init__()\n",
        "\n",
        "        im_channels = model_config['im_channels']\n",
        "        self.down_channels = model_config['down_channels']\n",
        "        self.mid_channels = model_config['mid_channels']\n",
        "        self.t_emb_dim = model_config['time_emb_dim']\n",
        "        self.down_sample = model_config['down_sample']\n",
        "        self.num_down_layers = model_config['num_down_layers']\n",
        "        self.num_mid_layers = model_config['num_mid_layers']\n",
        "        self.num_up_layers = model_config['num_up_layers']\n",
        "\n",
        "        assert self.mid_channels[0] == self.down_channels[-1]\n",
        "        assert self.mid_channels[-1] == self.down_channels[-2]\n",
        "        assert len(self.down_sample) == len(self.down_channels) - 1\n",
        "\n",
        "        self.t_proj = nn.Sequential(\n",
        "            nn.Linear(self.t_emb_dim, self.t_emb_dim),\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(self.t_emb_dim, self.t_emb_dim)\n",
        "        )\n",
        "\n",
        "        self.up_sample = list(reversed(self.down_sample))\n",
        "        self.conv_in = nn.Conv2d(im_channels, self.down_channels[0], kernel_size=3, padding=1)\n",
        "\n",
        "        self.downs = nn.ModuleList([\n",
        "            DownBlock(self.down_channels[i], self.down_channels[i + 1], self.t_emb_dim,\n",
        "                      down_sample=self.down_sample[i], num_layers=self.num_down_layers)\n",
        "            for i in range(len(self.down_channels) - 1)\n",
        "        ])\n",
        "\n",
        "        self.mids = nn.ModuleList([\n",
        "            MidBlock(self.mid_channels[i], self.mid_channels[i + 1], self.t_emb_dim,\n",
        "                     num_layers=self.num_mid_layers)\n",
        "            for i in range(len(self.mid_channels) - 1)\n",
        "        ])\n",
        "\n",
        "        self.ups = nn.ModuleList([\n",
        "            UpBlock(self.down_channels[i] * 2, self.down_channels[i - 1] if i != 0 else 16,\n",
        "                    self.t_emb_dim, up_sample=self.down_sample[i], num_layers=self.num_up_layers)\n",
        "            for i in reversed(range(len(self.down_channels) - 1))\n",
        "        ])\n",
        "\n",
        "        self.norm_out = nn.GroupNorm(8, 16)\n",
        "        self.conv_out = nn.Conv2d(16, im_channels, kernel_size=3, padding=1)\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        t = torch.as_tensor(t).long().view(-1)\n",
        "        out = self.conv_in(x)\n",
        "\n",
        "        down_outs = []\n",
        "        for down in self.downs:\n",
        "            down_outs.append(out)\n",
        "            out = down(out, t)\n",
        "\n",
        "        for mid in self.mids:\n",
        "            out = mid(out, t)\n",
        "\n",
        "        for up in self.ups:\n",
        "            down_out = down_outs.pop()\n",
        "            out = up(out, down_out, t)\n",
        "\n",
        "        out = self.norm_out(out)\n",
        "        out = nn.SiLU()(out)\n",
        "        out = self.conv_out(out)\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NtXhCiRourk"
      },
      "source": [
        "#### Noise Scheduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gFp95g7LMZru"
      },
      "outputs": [],
      "source": [
        "class LinearNoiseScheduler:\n",
        "    def __init__(self, num_timesteps, beta_start, beta_end, device):\n",
        "        self.num_timesteps = num_timesteps\n",
        "        self.beta_start = beta_start\n",
        "        self.beta_end = beta_end\n",
        "        self.device = device\n",
        "\n",
        "        self.betas = torch.linspace(beta_start, beta_end, num_timesteps).to(device)\n",
        "        self.alphas = 1. - self.betas\n",
        "        self.alpha_cum_prod = torch.cumprod(self.alphas, dim=0)\n",
        "        self.sqrt_alpha_cum_prod = torch.sqrt(self.alpha_cum_prod)\n",
        "        self.sqrt_one_minus_alpha_cum_prod = torch.sqrt(1 - self.alpha_cum_prod)\n",
        "\n",
        "    def add_noise(self, original, noise, t):\n",
        "        batch_size = original.shape[0]\n",
        "        sqrt_alpha_cum_prod = self.sqrt_alpha_cum_prod[t].reshape(batch_size, *([1] * (len(original.shape) - 1)))\n",
        "        sqrt_one_minus_alpha_cum_prod = self.sqrt_one_minus_alpha_cum_prod[t].reshape(batch_size, *([1] * (len(original.shape) - 1)))\n",
        "        return sqrt_alpha_cum_prod * original + sqrt_one_minus_alpha_cum_prod * noise\n",
        "\n",
        "    def sample_prev_timestep(self, xt, noise_pred, t):\n",
        "        batch_size = xt.shape[0]\n",
        "        sqrt_one_minus_alpha_cum_prod_t = self.sqrt_one_minus_alpha_cum_prod[t].reshape(batch_size, *([1] * (len(xt.shape) - 1)))\n",
        "        alpha_cum_prod_t = self.alpha_cum_prod[t].reshape(batch_size, *([1] * (len(xt.shape) - 1)))\n",
        "\n",
        "        x0 = (xt - sqrt_one_minus_alpha_cum_prod_t * noise_pred) / torch.sqrt(alpha_cum_prod_t)\n",
        "        x0 = torch.clamp(x0, -1., 1.)\n",
        "\n",
        "        mean = xt - ((self.betas[t].reshape(batch_size, *([1] * (len(xt.shape) - 1))) * noise_pred) / sqrt_one_minus_alpha_cum_prod_t)\n",
        "        mean = mean / torch.sqrt(self.alphas[t].reshape(batch_size, *([1] * (len(xt.shape) - 1))))\n",
        "\n",
        "        if t[0] == 0:\n",
        "            return mean, x0\n",
        "        else:\n",
        "            variance = (1 - self.alpha_cum_prod[t - 1]) / (1.0 - self.alpha_cum_prod[t])\n",
        "            variance = variance * self.betas[t]\n",
        "            sigma = variance ** 0.5\n",
        "            z = torch.randn(xt.shape, device=self.device)\n",
        "            return mean + sigma.reshape(batch_size, *([1] * (len(xt.shape) - 1))) * z, x0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLCp5bO85QBj"
      },
      "source": [
        "### Training the Diffusion Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a35DnHd3o2df"
      },
      "source": [
        "#### Training and Model Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "etd-04vkM0FO"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# Configuration\n",
        "diffusion_config = {\n",
        "    'num_timesteps': 1000,\n",
        "    'beta_start': 0.0001,\n",
        "    'beta_end': 0.02,\n",
        "    'device' : device,\n",
        "}\n",
        "model_config = {\n",
        "    'im_channels': 1,\n",
        "    'im_size': 28,\n",
        "    'down_channels': [32, 64, 128, 256],\n",
        "    'mid_channels': [256, 256, 128],\n",
        "    'down_sample': [True, True, False],\n",
        "    'time_emb_dim': 128,\n",
        "    'num_down_layers': 4,\n",
        "    'num_mid_layers': 4,\n",
        "    'num_up_layers': 4,\n",
        "    'num_heads': 4,\n",
        "}\n",
        "train_config = {\n",
        "    'task_name': 'mnist_diffusion',\n",
        "    'batch_size': 256,\n",
        "    'num_epochs': 50,\n",
        "    'lr': 1e-4,\n",
        "    'ckpt_name': 'ddpm_ckpt.pth',\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_-wdmqF5ho6"
      },
      "source": [
        "#### Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L1oqcyOGe2sz"
      },
      "outputs": [],
      "source": [
        "gc.collect()\n",
        "with torch.no_grad():\n",
        "    torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZBUJSO7jDOQY"
      },
      "outputs": [],
      "source": [
        "def train(model, train_loader, scheduler, optimizer, criterion, num_epochs, device, save_path):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        epoch_loss = 0\n",
        "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            images = batch[0].to(device)\n",
        "            noise = torch.randn_like(images).to(device)\n",
        "            timesteps = torch.randint(0, scheduler.num_timesteps, (images.shape[0],)).to(device)\n",
        "\n",
        "            noisy_images = scheduler.add_noise(images, noise, timesteps)\n",
        "            predicted_noise = model(noisy_images, timesteps)\n",
        "\n",
        "            loss = criterion(predicted_noise, noise)\n",
        "            loss.backward()\n",
        "\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "        avg_loss = epoch_loss / len(train_loader)\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n",
        "        torch.save(model.state_dict(), save_path)\n",
        "\n",
        "    print(\"Training completed.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJEI5PkkBFnV",
        "outputId": "a2f0ff21-61de-4bc4-fea7-56256279183f"
      },
      "outputs": [],
      "source": [
        "# Setup\n",
        "scheduler = LinearNoiseScheduler(**diffusion_config)\n",
        "\n",
        "# Dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "dataloader = DataLoader(dataset, batch_size=train_config['batch_size'], shuffle=True, num_workers=4)\n",
        "\n",
        "# Model\n",
        "model = Unet(model_config).to(device)\n",
        "optimizer = Adam(model.parameters(), lr=train_config['lr'])\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# Training\n",
        "save_path = os.path.join(train_config['task_name'], train_config['ckpt_name'])\n",
        "os.makedirs(train_config['task_name'], exist_ok=True)\n",
        "train(model, dataloader, scheduler, optimizer, criterion, train_config['num_epochs'], device, save_path)\n",
        "\n",
        "\n",
        "print(\"Training completed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_AYplnpNHiP"
      },
      "source": [
        "### Sampling the Diffusion Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X--FYqdrK3e0"
      },
      "outputs": [],
      "source": [
        "def sample(model, scheduler, num_samples, image_size, channels, device):\n",
        "    \"\"\"\n",
        "    Generate samples from a trained diffusion model.\n",
        "\n",
        "    :param model: The trained UNet model.\n",
        "    :param scheduler: The noise scheduler.\n",
        "    :param num_samples: Number of images to generate.\n",
        "    :param image_size: Size of the images to generate.\n",
        "    :param channels: Number of channels in the images.\n",
        "    :param device: The device to run the sampling on.\n",
        "    :return: Generated images as a tensor.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # Start from pure noise\n",
        "        x = torch.randn(num_samples, channels, image_size, image_size).to(device)\n",
        "\n",
        "        # Iteratively denoise the images\n",
        "        for i in tqdm(reversed(range(scheduler.num_timesteps)), desc='Sampling'):\n",
        "            t = torch.full((num_samples,), i, device=device, dtype=torch.long)\n",
        "            predicted_noise = model(x, t)\n",
        "\n",
        "            # Compute the previous noisy sample x_t -> x_t-1\n",
        "            x, _ = scheduler.sample_prev_timestep(x, predicted_noise, t)\n",
        "\n",
        "    # Rescale and clamp the image to be in [0, 1] range\n",
        "    x = (x.clamp(-1, 1) + 1) / 2\n",
        "    return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x6x8ZPtHmp2o"
      },
      "source": [
        "####Sampling from the trained model  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "id": "Ca14Ov7-KYJY",
        "outputId": "7018699c-c3b1-4e56-e319-0c39ad7b9bd0"
      },
      "outputs": [],
      "source": [
        "# After training, generate samples\n",
        "num_samples = 16\n",
        "generated_samples = sample(model, scheduler, num_samples, model_config['im_size'], model_config['im_channels'], device)\n",
        "\n",
        "# Create a grid of images\n",
        "img_grid = make_grid(generated_samples, nrow=4)\n",
        "\n",
        "# Convert to PIL Image\n",
        "img_grid_pil = torchvision.transforms.ToPILImage()(img_grid)\n",
        "\n",
        "# Save the image grid\n",
        "img_grid_pil.save(os.path.join(train_config['task_name'], 'generated_samples.png'))\n",
        "\n",
        "print(f\"Generated samples saved to {os.path.join(train_config['task_name'], 'generated_samples.png')}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHHncMsXmxg3"
      },
      "source": [
        "#### Sampling from a checkpointed model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dAiptnmQl9kV"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision.utils import make_grid\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import imageio\n",
        "\n",
        "def load_model_from_checkpoint(model, checkpoint_path, device):\n",
        "    \"\"\"\n",
        "    Load a model from a checkpoint, ignoring mismatched keys.\n",
        "\n",
        "    :param model: The model architecture (uninitialized).\n",
        "    :param checkpoint_path: Path to the checkpoint file.\n",
        "    :param device: The device to load the model onto.\n",
        "    :return: The loaded model.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(checkpoint_path):\n",
        "        raise FileNotFoundError(f\"No checkpoint found at {checkpoint_path}\")\n",
        "\n",
        "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "\n",
        "    # If the checkpoint contains the full model, not just the state dict\n",
        "    if isinstance(checkpoint, dict) and 'state_dict' in checkpoint:\n",
        "        checkpoint = checkpoint['state_dict']\n",
        "\n",
        "    # Filter out unexpected keys\n",
        "    model_dict = model.state_dict()\n",
        "    pretrained_dict = {k: v for k, v in checkpoint.items() if k in model_dict}\n",
        "\n",
        "    # Update model state dict\n",
        "    model_dict.update(pretrained_dict)\n",
        "    model.load_state_dict(model_dict)\n",
        "\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    print(f\"Loaded checkpoint from {checkpoint_path}\")\n",
        "    print(f\"Loaded {len(pretrained_dict)} / {len(model_dict)} layers\")\n",
        "\n",
        "    return model\n",
        "\n",
        "def sample_and_create_gif(checkpoint_path, model, scheduler, num_samples, image_size, channels, device, output_dir, num_gif_frames=100):\n",
        "    \"\"\"\n",
        "    Generate samples from a trained diffusion model checkpoint and create a GIF of the process.\n",
        "\n",
        "    :param checkpoint_path: Path to the model checkpoint.\n",
        "    :param model: The UNet model architecture (uninitialized).\n",
        "    :param scheduler: The noise scheduler.\n",
        "    :param num_samples: Number of images to generate.\n",
        "    :param image_size: Size of the images to generate.\n",
        "    :param channels: Number of channels in the images.\n",
        "    :param device: The device to run the sampling on.\n",
        "    :param output_dir: Directory to save the output files.\n",
        "    :param num_gif_frames: Number of frames to include in the GIF.\n",
        "    :return: Path to the created GIF.\n",
        "    \"\"\"\n",
        "    model = load_model_from_checkpoint(model, checkpoint_path, device)\n",
        "\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    frames = []\n",
        "    total_steps = scheduler.num_timesteps\n",
        "    save_interval = max(total_steps // num_gif_frames, 1)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Start from pure noise\n",
        "        x = torch.randn(num_samples, channels, image_size, image_size).to(device)\n",
        "\n",
        "        # Iteratively denoise the images\n",
        "        for i in tqdm(reversed(range(total_steps)), desc='Sampling'):\n",
        "            t = torch.full((num_samples,), i, device=device, dtype=torch.long)\n",
        "            predicted_noise = model(x, t)\n",
        "\n",
        "            # Compute the previous noisy sample x_t -> x_t-1\n",
        "            x, _ = scheduler.sample_prev_timestep(x, predicted_noise, t)\n",
        "\n",
        "            # Save frame for GIF\n",
        "            if i % save_interval == 0 or i == total_steps - 1 or i == 0:\n",
        "                # Rescale and clamp the image to be in [0, 1] range\n",
        "                frame = (x.clamp(-1, 1) + 1) / 2\n",
        "                grid = make_grid(frame, nrow=int(num_samples**0.5))\n",
        "                img = torchvision.transforms.ToPILImage()(grid)\n",
        "                frames.append(img)\n",
        "\n",
        "    # Save the GIF\n",
        "    gif_path = os.path.join(output_dir, 'diffusion_process.gif')\n",
        "    frames[0].save(gif_path, save_all=True, append_images=frames[1:], duration=100, loop=1)\n",
        "\n",
        "    # Save the final image\n",
        "    final_image_path = os.path.join(output_dir, 'final_samples.png')\n",
        "    frames[-1].save(final_image_path)\n",
        "\n",
        "    return gif_path, final_image_path\n",
        "\n",
        "# Example usage in main function:\n",
        "def gen_samples():\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    # Configuration\n",
        "    diffusion_config = {\n",
        "        'num_timesteps': 1000,\n",
        "        'beta_start': 0.0001,\n",
        "        'beta_end': 0.02,\n",
        "        'device' : device,\n",
        "    }\n",
        "    model_config = {\n",
        "        'im_channels': 1,\n",
        "        'im_size': 28,\n",
        "        'down_channels': [32, 64, 128, 256],\n",
        "        'mid_channels': [256, 256, 128],\n",
        "        'down_sample': [True, True, False],\n",
        "        'time_emb_dim': 128,\n",
        "        'num_down_layers': 4,\n",
        "        'num_mid_layers': 4,\n",
        "        'num_up_layers': 4,\n",
        "        'num_heads': 4,\n",
        "    }\n",
        "    train_config = {\n",
        "        'task_name': 'mnist_diffusion',\n",
        "        'batch_size': 256,\n",
        "        'num_epochs': 50,\n",
        "        'lr': 1e-4,\n",
        "        'ckpt_name': 'ddpm_ckpt.pth',\n",
        "    }\n",
        "\n",
        "\n",
        "    # Initialize model and scheduler\n",
        "    model = Unet(model_config)\n",
        "    scheduler = LinearNoiseScheduler(**diffusion_config)\n",
        "\n",
        "    # Set up checkpoint path and output directory\n",
        "    # checkpoint_path = os.path.join(train_config['task_name'], train_config['ckpt_name'])\n",
        "    checkpoint_path = \"./pretrained/trained_ddpm_ckpt.pth\"\n",
        "    output_dir = os.path.join(train_config['task_name'], 'visualization')\n",
        "\n",
        "    # Generate samples and create GIF\n",
        "    num_samples = 25\n",
        "    gif_path, final_image_path = sample_and_create_gif(\n",
        "        checkpoint_path, model, scheduler, num_samples,\n",
        "        model_config['im_size'], model_config['im_channels'],\n",
        "        device, output_dir\n",
        "    )\n",
        "\n",
        "    print(f\"Diffusion process GIF saved to {gif_path}\")\n",
        "    print(f\"Final samples saved to {final_image_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7jkPAx3Em__7",
        "outputId": "1e07411f-7fe5-4b98-ccce-c8b8899e7ecf"
      },
      "outputs": [],
      "source": [
        "gen_samples()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8MJH9XwramF"
      },
      "source": [
        "Note how long this takes to generate? Maybe in the next lesson we can speed things up."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 169
        },
        "id": "KhTaEseXnslZ",
        "outputId": "26fd3632-5ea6-4c94-b94f-6b28f0cd795f"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Image, display\n",
        "import ipywidgets as widgets\n",
        "display(Image(filename='/content/mnist_diffusion/visualization/diffusion_process.gif'))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
