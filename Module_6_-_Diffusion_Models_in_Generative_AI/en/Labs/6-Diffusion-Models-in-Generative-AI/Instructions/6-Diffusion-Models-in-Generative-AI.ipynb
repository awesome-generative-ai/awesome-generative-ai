{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework: Module 6\n",
    "\n",
    "In this homework, we'll explore key concepts from generative AI, including GANs, diffusion models, and CLIP. We'll use pre-trained models from Hugging Face to complete various tasks.\n",
    "\n",
    "## Setup\n",
    "\n",
    "First, let's install the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers torch torchvision diffusers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from diffusers import StableDiffusionPipeline\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Exploring CLIP\n",
    "\n",
    "CLIP (Contrastive Language-Image Pre-training) is a model that can understand and compare images and text. Let's use it to perform image-text similarity tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_clip_model():\n",
    "    model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    return model.to(device), processor\n",
    "\n",
    "clip_model, clip_processor = setup_clip_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Image-Text Similarity\n",
    "\n",
    "Complete the following function to compute the similarity between an image and a list of text prompts using CLIP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_image_text_similarity(model, processor, image, texts):\n",
    "    # TODO: Implement the function to compute similarity scores\n",
    "    # Hint: Use the processor to prepare inputs and the model to get logits\n",
    "    # Return the similarity scores\n",
    "    pass\n",
    "\n",
    "# Test your function\n",
    "image_url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "texts = [\"a photo of a cat\", \"a photo of a dog\", \"a photo of a giraffe\"]\n",
    "\n",
    "# TODO: Load the image and compute similarities\n",
    "# Print the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Exploring Diffusion Models\n",
    "\n",
    "Diffusion models are a class of generative models that have shown impressive results in image generation. We'll use a pre-trained Stable Diffusion model to generate images from text prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_diffusion_model():\n",
    "    model_id = \"runwayml/stable-diffusion-v1-5\"\n",
    "    pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\n",
    "    return pipe.to(device)\n",
    "\n",
    "diffusion_pipe = setup_diffusion_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Text-to-Image Generation\n",
    "\n",
    "Complete the following function to generate images from text prompts using the Stable Diffusion model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_image_from_text(pipe, prompt, num_images=1):\n",
    "    # TODO: Implement the function to generate images from text\n",
    "    # Hint: Use the pipe object to generate images\n",
    "    # Return the generated images\n",
    "    pass\n",
    "\n",
    "# Test your function\n",
    "prompt = \"A serene landscape with mountains and a lake at sunset\"\n",
    "\n",
    "# TODO: Generate images and display them\n",
    "# Hint: Use plt.imshow() to display the images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Exploring Guidance Scale\n",
    "\n",
    "The guidance scale in Stable Diffusion models controls the trade-off between image quality and prompt adherence. Implement a function to compare different guidance scales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_guidance_scales(pipe, prompt, scales=[5.0, 7.5, 10.0]):\n",
    "    # TODO: Implement the function to generate images with different guidance scales\n",
    "    # Hint: Use the guidance_scale parameter in the pipe\n",
    "    # Generate and display images side by side\n",
    "    pass\n",
    "\n",
    "# Test your function\n",
    "prompt = \"A futuristic city skyline at night\"\n",
    "compare_guidance_scales(diffusion_pipe, prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Combining CLIP and Diffusion Models\n",
    "\n",
    "Now, let's combine what we've learned about CLIP and diffusion models to create a more advanced application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: Image Generation and Evaluation\n",
    "\n",
    "Implement a function that generates an image using the diffusion model and then evaluates how well it matches the original prompt using CLIP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_evaluate(diffusion_pipe, clip_model, clip_processor, prompt):\n",
    "    # TODO: Implement the function to generate an image and evaluate it\n",
    "    # 1. Generate an image using the diffusion model\n",
    "    # 2. Use CLIP to compute the similarity between the generated image and the original prompt\n",
    "    # 3. Display the image and print the similarity score\n",
    "    pass\n",
    "\n",
    "# Test your function\n",
    "prompt = \"A colorful hot air balloon floating over a field of flowers\"\n",
    "generate_and_evaluate(diffusion_pipe, clip_model, clip_processor, prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this homework, you've explored key concepts in generative AI, including:\n",
    "\n",
    "1. Using CLIP for image-text similarity tasks\n",
    "2. Generating images with diffusion models\n",
    "3. Understanding the impact of guidance scale in image generation\n",
    "4. Combining CLIP and diffusion models for image generation and evaluation\n",
    "\n",
    "These techniques are fundamental to many advanced applications in computer vision and natural language processing. Keep exploring and experimenting with these powerful models!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
